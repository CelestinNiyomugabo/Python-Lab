{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a1cd838",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "## Ridge Regression: Effect of λ on Bias–Variance Tradeoff\n",
    "\n",
    "We estimate the regression coefficients by minimizing:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2\n",
    "$$\n",
    "\n",
    "where $ \\lambda $ controls the strength of the penalty on coefficient size.\n",
    "\n",
    "---\n",
    "\n",
    "### (a) Effect of $ \\lambda $ on **Training RSS**\n",
    "\n",
    "**Answer:** iii. Steadily increase.\n",
    "\n",
    "**Justification:**\n",
    "\n",
    "- When $ \\lambda = 0 $, we recover **ordinary least squares (OLS)**, which minimizes training RSS.\n",
    "- As $ \\lambda $ increases, the penalty forces coefficients $ \\beta_j $ to shrink.\n",
    "- This reduces model flexibility, so **training error increases**.\n",
    "\n",
    "---\n",
    "\n",
    "### (b) Effect of $ \\lambda $ on **Test RSS**\n",
    "\n",
    "**Answer:** ii. Decrease initially, and then eventually start increasing in a U shape.\n",
    "\n",
    "**Justification:**\n",
    "\n",
    "- Small $ \\lambda $: overfitting → high variance → high test error.\n",
    "- Moderate $ \\lambda $: better generalization → test error decreases.\n",
    "- Large $ \\lambda $: underfitting → high bias → test error increases.\n",
    "- So, test RSS follows a **U-shape**.\n",
    "\n",
    "---\n",
    "\n",
    "### (c) Effect of $ \\lambda $ on **Variance**\n",
    "\n",
    "**Answer:** iii. Steadily decrease.\n",
    "\n",
    "**Justification:**\n",
    "\n",
    "- As $ \\lambda $ increases, the model becomes less sensitive to training data.\n",
    "- Coefficients shrink toward zero, making the model more stable.\n",
    "- Thus, **model variance decreases** steadily.\n",
    "\n",
    "---\n",
    "\n",
    "### (d) Effect of $ \\lambda $ on **(Squared) Bias**\n",
    "\n",
    "**Answer:** iii. Steadily increase.\n",
    "\n",
    "**Justification:**\n",
    "\n",
    "- As $ \\lambda $ increases, the model is less able to capture the true relationship.\n",
    "- Predictions deviate more from the actual function.\n",
    "- Therefore, **bias increases** with $ \\lambda $.\n",
    "\n",
    "---\n",
    "\n",
    "### (e) Effect of $ \\lambda $ on **Irreducible Error**\n",
    "\n",
    "**Answer:** v. Remain constant.\n",
    "\n",
    "**Justification:**\n",
    "\n",
    "- Irreducible error is due to noise in the data (e.g., measurement error).\n",
    "- It is **independent** of the model or choice of $ \\lambda $.\n",
    "- Hence, it **remains constant**.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary \n",
    "\n",
    "| Part | Quantity             | Answer | Description |\n",
    "|------|----------------------|--------|-------------|\n",
    "| (a)  | Training RSS         | iii    | Increases as $ \\lambda $ increases |\n",
    "| (b)  | Test RSS             | ii     | U-shaped curve |\n",
    "| (c)  | Variance             | iii    | Steadily decreases |\n",
    "| (d)  | Squared Bias         | iii    | Steadily increases |\n",
    "| (e)  | Irreducible Error    | v      | Remains constant |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2938f99",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57f6f8a0",
   "metadata": {},
   "source": [
    "# Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42129a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ISLP import load_data\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# 2. Load data\n",
    "College = load_data('College')\n",
    "X = College.drop('Apps', axis=1)\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "y = College['Apps']\n",
    "\n",
    "# 3. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb30b31",
   "metadata": {},
   "source": [
    "### (b) OLS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c695ac1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "642753.8976533434"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "mse_lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8330ba6b",
   "metadata": {},
   "source": [
    "### (c) Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4248e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "653969.1227226362"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas = np.logspace(-3, 5, 100)\n",
    "ridge = RidgeCV(alphas=alphas, scoring='neg_mean_squared_error', cv=10)\n",
    "ridge.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "mse_ridge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71751d5e",
   "metadata": {},
   "source": [
    "### (d) Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd781b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(831509.2476266614, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso = LassoCV(alphas=None, cv=10, max_iter=10000)\n",
    "lasso.fit(X_train, y_train)\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "n_nonzero = np.sum(lasso.coef_ != 0)\n",
    "mse_lasso, n_nonzero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b04b33",
   "metadata": {},
   "source": [
    "### (e) PCR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "404a1ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Standardize + PCA + Linear Regression\n",
    "mse_pcr = []\n",
    "for m in range(1, X_train.shape[1]+1):\n",
    "    pca = PCA(n_components=m)\n",
    "    X_train_pca = pca.fit_transform(StandardScaler().fit_transform(X_train))\n",
    "    X_test_pca = pca.transform(StandardScaler().fit_transform(X_test))\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_pca, y_train)\n",
    "    y_pred = model.predict(X_test_pca)\n",
    "    mse_pcr.append(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "best_m_pcr = np.argmin(mse_pcr) + 1\n",
    "best_m_pcr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3c12d4",
   "metadata": {},
   "source": [
    "### (f) PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02181d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_pls = []\n",
    "for m in range(1, X_train.shape[1]+1):\n",
    "    pls = PLSRegression(n_components=m)\n",
    "    pls.fit(StandardScaler().fit_transform(X_train), y_train)\n",
    "    y_pred = pls.predict(StandardScaler().fit_transform(X_test))\n",
    "    mse_pls.append(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "best_m_pls = np.argmin(mse_pls) + 1\n",
    "best_m_pls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e3e40f",
   "metadata": {},
   "source": [
    "### (g) Compare and Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a151ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Model       Test MSE  Best Param\n",
      "0    OLS  642753.897653           -\n",
      "1  Ridge  653969.122723   23.101297\n",
      "2  Lasso  831509.247627  6 non-zero\n",
      "3    PCR  958162.737647          17\n",
      "4    PLS  941216.372794           9\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Model': ['OLS', 'Ridge', 'Lasso', 'PCR', 'PLS'],\n",
    "    'Test MSE': [mse_lr, mse_ridge, mse_lasso, min(mse_pcr), min(mse_pls)],\n",
    "    'Best Param': ['-', ridge.alpha_, f'{n_nonzero} non-zero', best_m_pcr, best_m_pls]\n",
    "})\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0df62ed",
   "metadata": {},
   "source": [
    "| Model | Test MSE       | Best Param    |\n",
    "| ----- | -------------- | ------------- |\n",
    "| OLS   | **642,753.90** | –             |\n",
    "| Ridge | 653,969.12     | 23.10         |\n",
    "| Lasso | 831,509.25     | 6 non-zero    |\n",
    "| PCR   | 958,162.74     | 17 components |\n",
    "| PLS   | 941,216.37     | 9 components  |\n",
    "\n",
    "\n",
    "1. Best Performance: OLS\n",
    "The ordinary least squares (OLS) model yielded the lowest test MSE, meaning it had the best out-of-sample predictive accuracy in this case.\n",
    "\n",
    "This suggests that regularization was not essential for this specific dataset — multicollinearity or overfitting may not have been severe.\n",
    "\n",
    "2. Ridge Regression\n",
    "Ridge regression performed nearly as well as OLS, with a slightly higher MSE.\n",
    "\n",
    "It selected a moderate penalty term (λ ≈ 23.1), which shrinks all coefficients but retains them.\n",
    "\n",
    "It’s more robust to multicollinearity, so it might be preferred in situations with noisy predictors — even if not optimal here.\n",
    "\n",
    "3. Lasso Regression\n",
    "Lasso produced a higher test error, but also selected only 6 non-zero coefficients, meaning it performs automatic feature selection.\n",
    "\n",
    "It’s a good choice if you want a sparse, interpretable model, even if predictive accuracy slightly drops.\n",
    "\n",
    "4. PCR & PLS\n",
    "Both PCR and PLS performed worse than OLS/Ridge/Lasso.\n",
    "\n",
    "PCR used 17 principal components (unsupervised), while PLS used 9 (supervised).\n",
    "\n",
    "This might indicate that:\n",
    "\n",
    "The outcome (Apps) isn’t strongly aligned with principal components.\n",
    "\n",
    "Or, dimensionality reduction lost some important predictive information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e4c064",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
