
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Data Mining - AUCA</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 40px 10%;
            line-height: 1.8;
            background-color: #fdfdfd;
            color: #333;
        }
        h1, h2 {
            color: #1a5276;
        }
        a {
            color: #2874a6;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        ul {
            padding-left: 20px;
        }
    </style>
</head>
<body>
    <h1>Summary of what we have covered</h1>
    <h3>Celestin Niyomugabo - 100883</h3>
    <p>This course taught me how to apply key statistical learning and data mining methods in Python to solve real-world problems. I learned to distinguish between different types of learning tasks, such as regression, classification, and clustering, and to select appropriate models for each based on data type, structure, and objective. Through hands-on labs, I developed practical skills in training, validating, and interpreting models using Python. I gained valuable experience in using model evaluation techniques like cross-validation, bootstrapping, and regularization to improve model performance. I also explored how ensemble learning methods and deep learning architectures enhance predictive accuracy in complex scenarios. Most importantly, I learned how these techniques connect to big data analytics by enabling scalable, efficient, and insightful analysis of large and diverse datasets.</p>


    <h2>Table of Contents</h2>
    <ul>
        <li><a href="#chapter2">Chapter 2: Statistical Learning</a></li>
        <li><a href="#chapter3">Chapter 3: Linear Regression</a></li>
        <li><a href="#chapter4">Chapter 4: Classification</a></li>
        <li><a href="#chapter5">Chapter 5: Resampling Methods</a></li>
        <li><a href="#chapter6">Chapter 6: Model Selection & Regularization</a></li>
        <li><a href="#chapter8">Chapter 8: Tree-Based Methods</a></li>
        <li><a href="#chapter9">Chapter 9: Support Vector Machines</a></li>
        <li><a href="#chapter10">Chapter 10: Deep Learning</a></li>
        <li><a href="#chapter12">Chapter 12: Unsupervised Learning</a></li>
    </ul>

    <h2 id="chapter2">Chapter 2: Statistical Learning</h2>
    <p>This chapter introduced supervised and unsupervised learning, showing how machine learning models identify patterns from data. In supervised learning, models are trained on input features with known outcomes. In unsupervised learning, models try to detect patterns without labeled outcomes. The chapter explains that simpler models may underfit while overly complex models may overfit, highlighting the importance of the bias-variance tradeoff. Through Python examples, I learned how to use NumPy arrays for data manipulation and visualization using matplotlib. The K-Nearest Neighbors (KNN) algorithm is presented as a simple yet effective technique that predicts outcomes based on the proximity of data points in feature space. KNN works best when features are scaled and the data has natural groupings, as demonstrated through scatter plots, reshaping arrays, and basic numerical operations in Python.
    <p><strong>Lab:</strong> <a href="https://github.com/CelestinNiyomugabo/Python-Lab/blob/a1b944f274f5441ace446eb673ca9228580b9230/Lab%20Chap%202.py">Chapter 2 Lab</a></p>

    <h2 id="chapter3">Chapter 3: Linear Regression</h2>
    <p>This chapter focused on using linear regression to predict continuous outcomes. It begins with simple linear regression, modeling the relationship between one predictor and the outcome, and shows how to fit models using statsmodels. Diagnostic plots help evaluate residual behavior, fitted values, and leverage, ensuring assumptions like linearity and constant variance are reasonably met. The chapter then expands to multiple linear regression, including all predictors to assess their combined influence. Using Variance Inflation Factor (VIF), it introduces multicollinearity checks. The chapter also covers adding interaction terms and polynomial terms using ModelSpec, and compares models using ANOVA. Finally, it explores regression with qualitative predictors, showing how to encode and include interactions between categorical and continuous variables for richer modeling.
    <p><strong>Lab:</strong> <a href="https://github.com/CelestinNiyomugabo/Python-Lab/blob/a1b944f274f5441ace446eb673ca9228580b9230/Lab%20Chap%203.py">Chapter 3 Lab</a></p>

    <h2 id="chapter4">Chapter 4: Classification</h2>
    <p>In this chapter we covered different approaches to classifying categorical outcomes. 
        <ul>
            <li><strong>Logistic Regression:</strong> estimates the likelihood of an event occurring based on input features and is suitable when the relationship between the predictors and the log-odds of the outcome is approximately linear. </li>
            <li><strong>Linear Discriminant Analysis:</strong> performs well when classes are normally distributed with equal variance across groups. </li>
            <li><strong>Quadratic Discriminant Analysis:</strong> extends this by allowing different variances, making it better suited to more flexible scenarios. </li>
            <li><strong>K-Nearest Neighbors:</strong> classifies based on the closest training examples, requiring feature scaling to ensure distance metrics are meaningful. </li>
            <li><strong>Naive Bayes:</strong> applies conditional probability, handling high-dimensional inputs effectively when predictor independence is reasonably assumed. </li>
        </ul>
    <p><strong>Lab:</strong> <a href="https://github.com/CelestinNiyomugabo/Python-Lab/blob/a1b944f274f5441ace446eb673ca9228580b9230/Lab%20Chap%204.py">Chapter 4 Lab</a></p>

    <h2 id="chapter5">Chapter 5: Resampling Methods</h2>
   <p>In this chapter we covered cross-validation and bootstrap techniques, essential for estimating how well models generalize to new data. These methods are especially useful when data is limited and a separate test set is not available. </p>
   <ul>
    <li><strong>Cross-validation:</strong> (such as k-fold and leave-one-out) ensures that the model evaluation is fair and less dependent on how the data is split. </li>
    <li><strong>Bootstrap method:</strong> allows us to quantify uncertainty in model estimates by repeatedly drawing samples with replacement from the observed data.</li>
   </ul>
   
    <p><strong>Lab:</strong> <a href="https://github.com/CelestinNiyomugabo/Python-Lab/blob/a1b944f274f5441ace446eb673ca9228580b9230/Lab%20Chap%205.py">Chapter 5 Lab</a></p>

    <h2 id="chapter6">Chapter 6: Model Selection & Regularization</h2>
    <p>This chapter focused on strategies for improving predictive performance and reducing overfitting are introduced. Regularization techniques like Ridge Regression and LASSO penalize overly complex models.</p>
     <ul>
        <li><strong>Ridge:</strong> is suitable when many variables are involved and some are highly correlated,</li>
        <li><strong>LASSO: </strong>helps when only a few variables are expected to matter by shrinking others to zero.</li>
        <li>Principal Component Regression (PCR) and Partial Least Squares (PLS) transform features into lower-dimensional spaces to remove redundancy before fitting the model.</li>
     </ul>
       
    <p><strong>Lab:</strong> <a href="https://github.com/CelestinNiyomugabo/Python-Lab/blob/a1b944f274f5441ace446eb673ca9228580b9230/Lab%20Chap%206.py">Chapter 6 Lab</a></p>

    <h2 id="chapter8">Chapter 8: Tree-Based Methods</h2>
    <p>Decision trees partition data into regions based on features, producing intuitive and interpretable models. These models are appealing due to their visual structure but can be unstable. 
        </p>
    <ul>
        <li><strong>Bagging</strong> and <strong>Random Forests</strong> overcome this by aggregating multiple trees, reducing variability and improving accuracy.  </li>
        <li><strong>Boosting</strong> builds trees sequentially, each focusing on correcting the errors of the previous.</li>
    </ul>
    <p>These ensemble methods excel in complex, high-dimensional settings where interactions and non-linearities dominate.</p>
    <p><strong>Lab:</strong> <a href="https://github.com/CelestinNiyomugabo/Python-Lab/blob/a1b944f274f5441ace446eb673ca9228580b9230/Lab%20Chap%208.py">Chapter 8 Lab</a></p>

    <h2 id="chapter9">Chapter 9: Support Vector Machines</h2>
    <p>SVMs aim to find the best separating hyperplane between classes by maximizing the margin (the distance between the decision boundary and the nearest data points). This makes them particularly effective when the data is clearly separable or lies near a margin. The use of kernels extends their power by enabling them to classify data that is not linearly separable. When classes are not well-separated in original space, the kernel trick allows transformation into a higher-dimensional space for better separation.</p>
    <p><strong>Lab:</strong> <a href="https://github.com/CelestinNiyomugabo/Python-Lab/blob/a1b944f274f5441ace446eb673ca9228580b9230/Lab%20Chap%209.py">Chapter 9 Lab</a></p>
    
    <h2 id="chapter10">Deep Learning</h2>
    <p>In this chapter we covered neural networks as flexible models capable of capturing complex, non-linear relationships in large datasets. Starting with single-layer perceptrons, we learnt how to builds up to multilayer networks trained using backpropagation and stochastic gradient descent. Convolutional Neural Networks (CNNs) are highlighted for image data due to their ability to learn spatial hierarchies, while Recurrent Neural Networks (RNNs) are introduced for sequential data such as text or time series. These methods require large volumes of data and computational resources, but they outperform traditional models in tasks such as document classification and image recognition.</p>
    <p><strong>Lab:</strong> <a href="https://github.com/CelestinNiyomugabo/Python-Lab/blob/a1b944f274f5441ace446eb673ca9228580b9230/Lab%20Chap%2010.py">Chapter 10 Lab</a></p>


    <h2 id="chapter12">Chapter 12: Unsupervised Learning</h2>
    <p>Unsupervised learning are used to uncover hidden patterns without predefined labels.</p>
    <ul>
        <li><strong>Principal Component Analysis (PCA):</strong> reduces dimensionality by identifying directions of maximum variance, helping simplify data while retaining essential structure.</li>
        <li><strong>K-Means clustering:</strong> groups observations into clusters of similar points, working well when the number of clusters is known in advance. </li>
        <li><strong>Hierarchical clustering: </strong> builds nested clusters and is suitable for exploratory analysis where the cluster structure is not known beforehand.</li>
    </ul>
        
     <p><strong>Lab:</strong> <a href="https://github.com/CelestinNiyomugabo/Python-Lab/blob/a1b944f274f5441ace446eb673ca9228580b9230/Lab%20Chap%2012.py">Chapter 12 Lab</a></p>


    <h2>Summary of algorithms</h2>
    <table class="table table-striped">
        <thead>
            <tr>
                <th>Category</th>
                <th>Algorithm</th>
                <th>Rationale</th>
            </tr>
        </thead>
        <tbody>
            <tr><td rowspan="4">Regression</td><td>Linear Regression</td><td>Models continuous outcomes with interpretability and simplicity.</td></tr>
            <tr><td>Ridge Regression</td><td>Handles multicollinearity by shrinking coefficients without eliminating them.</td></tr>
            <tr><td>LASSO Regression</td><td>Performs feature selection by driving some coefficients to zero.</td></tr>
            <tr><td>PCR / PLS</td><td>Reduces dimensionality before modeling to handle multicollinearity.</td></tr>
            <tr><td rowspan="8">Classification</td><td>Logistic Regression</td><td>Estimates class probabilities using a linear combination of features.</td></tr>
            <tr><td>LDA</td><td>Useful when class distributions are normal with equal variance.</td></tr>
            <tr><td>QDA</td><td>Allows different covariances across classes.</td></tr>
            <tr><td>Naive Bayes</td><td>Works well for high-dimensional data assuming feature independence.</td></tr>
            <tr><td>KNN</td><td>Simple method that classifies based on proximity in feature space.</td></tr>
            <tr><td>SVM</td><td>Effective for separating classes with a clear margin; adaptable via kernels.</td></tr>
            <tr><td>Random Forest</td><td>Handles high-dimensional data well, reduces overfitting by averaging many trees.</td></tr>
            <tr><td>Boosting</td><td>Builds strong classifiers by sequentially correcting errors of weak learners.</td></tr>
            <tr><td>Neural Networks</td><td>Capture complex nonlinear relationships; suitable for large-scale data, text, and image tasks.</td></tr>
            <tr><td rowspan="3">Unsupervised</td><td>K-Means</td><td>Groups similar data points into clusters for exploratory analysis.</td></tr>
            <tr><td>Hierarchical Clustering</td><td>Creates nested groupings; useful for dendrograms and unknown clusters.</td></tr>
            <tr><td>PCA</td><td>Reduces feature space while preserving maximum variance directions.</td></tr>
            
        </tbody>
    </table>
</body>
</html>
