{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2372c07",
   "metadata": {},
   "source": [
    "# MSDA 9213: Data Mining \n",
    "\n",
    "### 1. Discuss the difference between supervised and unsupervised methods\n",
    "\n",
    "- **Supervised Learning** involves labeled data. The algorithm learns a mapping from inputs (features) to outputs (labels). Examples: classification, regression.\n",
    "- **Unsupervised Learning** involves unlabeled data. The algorithm tries to identify hidden patterns or groupings. Examples: clustering, dimensionality reduction.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Describe two real-life applications:\n",
    "#### i. Classification\n",
    "- **Application**: Spam detection in emails.\n",
    "- **Response**: Binary variable (Spam or Not Spam).\n",
    "- **Predictors**: Email content, sender, subject, etc.\n",
    "- **Goal**: Prediction — we want to predict the category of new emails.\n",
    "\n",
    "#### ii. Regression\n",
    "- **Application**: Predicting house prices.\n",
    "- **Response**: Continuous variable (Price).\n",
    "- **Predictors**: Size, location, number of bedrooms, etc.\n",
    "- **Goal**: Prediction — we estimate a continuous outcome.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Why is Naïve Bayes so “naïve”?\n",
    "\n",
    "Naïve Bayes assumes **independence among predictors** given the class label, which is rarely true in real life. This “naïve” assumption simplifies computation and works surprisingly well even when the assumption is violated.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. What is the difference between KNN and K-means?\n",
    "\n",
    "| Feature           | KNN (K-Nearest Neighbors) | K-Means Clustering |\n",
    "|------------------|---------------------------|--------------------|\n",
    "| Type             | Supervised Learning       | Unsupervised Learning |\n",
    "| Goal             | Classification or regression | Clustering |\n",
    "| Input Required   | Labeled data              | Unlabeled data |\n",
    "| Output           | Class label or prediction | Cluster assignments |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. When is ridge regression favorable over Lasso regression?\n",
    "\n",
    "- Ridge regression is preferred when **many predictors have small/medium effects**, and we want to **shrink coefficients** but not eliminate them.\n",
    "- Unlike Lasso, Ridge **does not perform variable selection** but handles multicollinearity better.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. What is a confusion matrix and how does it work?\n",
    "\n",
    "A confusion matrix is a performance summary for classification models. It compares actual vs predicted classes:\n",
    "\n",
    "|               | Predicted Positive | Predicted Negative |\n",
    "|---------------|--------------------|--------------------|\n",
    "| Actual Positive | True Positive (TP)   | False Negative (FN) |\n",
    "| Actual Negative | False Positive (FP)  | True Negative (TN)  |\n",
    "\n",
    "It helps compute metrics like accuracy, precision, recall, and F1-score.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Cross-validation\n",
    "\n",
    "#### i. How k-fold cross-validation is implemented:\n",
    "- The dataset is split into *k* equal parts.\n",
    "- The model is trained on *k−1* folds and tested on the remaining fold.\n",
    "- This process is repeated *k* times, each fold serving once as the test set.\n",
    "- The performance is averaged across all k trials.\n",
    "\n",
    "#### ii. Advantages and disadvantages:\n",
    "\n",
    "**a. Compared to the validation set approach:**\n",
    "- **Advantage**: Uses the data more efficiently, leading to lower variance.\n",
    "- **Disadvantage**: More computationally intensive.\n",
    "\n",
    "**b. Compared to LOOCV (Leave-One-Out Cross-Validation):**\n",
    "- **Advantage**: Less computation than LOOCV.\n",
    "- **Disadvantage**: LOOCV has lower bias but higher variance.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Estimating standard deviation of prediction\n",
    "\n",
    "To estimate the standard deviation of our prediction (also known as the standard error of the prediction, $SE(\\hat{Y}_0)$) for a particular value of the predictor $X_0$, we consider two main sources of variability: uncertainty in the model's parameters and irreducible error.\n",
    "\n",
    "1.  **For Parametric Models (e.g., Linear Regression):**\n",
    "    For models with an explicit mathematical form, analytical formulas are typically available. For a simple linear regression predicting $Y$ at $X_0$:\n",
    "\n",
    "    $$SE(\\hat{Y}_0) = \\sqrt{\\hat{\\sigma}^2 \\left( 1 + \\frac{1}{n} + \\frac{(X_0 - \\bar{X})^2}{\\sum_{i=1}^n (X_i - \\bar{X})^2} \\right)}$$\n",
    "\n",
    "    Here, $\\hat{\\sigma}^2$ estimates the irreducible error, while the other terms account for the uncertainty in the estimated model parameters. Statistical software usually provides these directly.\n",
    "\n",
    "2.  **For Non-Parametric or Complex Models (e.g., Tree-based methods):**\n",
    "    For models without simple analytical solutions, resampling methods like the **bootstrap** are used:\n",
    "    * **Generate Bootstrap Samples:** Create many new datasets by sampling with replacement from the original training data.\n",
    "    * **Train and Predict:** Train your statistical learning method on each bootstrap sample and make a prediction for $X_0$. This yields multiple predictions ($\\hat{Y}_{0,1}, \\hat{Y}_{0,2}, \\dots, \\hat{Y}_{0,B}$).\n",
    "    * **Calculate Standard Deviation:** The standard deviation of these $B$ predictions is then used as the estimate for the standard deviation of your prediction. This primarily captures the uncertainty from the model's training process.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Two methods for variable selection and how they work\n",
    "\n",
    "1. **Forward Selection**:\n",
    "   - Start with no variables.\n",
    "   - Add predictors one by one that most improve model performance.\n",
    "   - Stop when adding more variables doesn’t significantly improve the model.\n",
    "\n",
    "2. **Lasso Regression**:\n",
    "   - Adds L1 penalty to the loss function.\n",
    "   - Shrinks some coefficients to exactly zero, effectively performing variable selection.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d004dd84",
   "metadata": {},
   "source": [
    "## Decision tree\n",
    "Decision trees consist of nodes (root, internal, leaf) representing tests or outcomes, and branches representing the test results. Pruning is essential to prevent overfitting, improve interpretability, and reduce computational cost by simplifying the tree. Decision trees are attractive for classification due to their high interpretability, ability to handle various data types without extensive pre-processing, and capacity to capture non-linear relationships.\n",
    "\n",
    "Improving an existing rule-based classifier with a data-driven approach involves collecting and preparing a large dataset, selecting an appropriate machine learning model, training it on the data, and then rigorously evaluating and tuning its performance using a separate validation set. To test the validity of the new model, its performance should be quantitatively assessed on an independent test set using metrics like accuracy, precision, and recall, compared directly against the existing rule-based system, and ideally, subjected to qualitative review by domain experts to ensure its real-world applicability and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a4956e",
   "metadata": {},
   "source": [
    "## Parameter tuning\n",
    "\n",
    "Parameter tuning, also known as hyperparameter tuning, is the process of finding the best combination of settings (hyperparameters) for a machine learning model that results in optimal performance on a given task."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
